"""
Daily Tech News Module
æ¯æ—¥æŠ€è¡“æ–°èæŠ“å–èˆ‡åˆ†æåŠŸèƒ½
"""

import os
import json
import threading
import time
from datetime import datetime
from typing import Optional
from dotenv import load_dotenv
from crewai import Agent, Task, Crew, Process, LLM
from crewai_tools import SerperDevTool, ScrapeWebsiteTool
from .prompt_manager import prompt_manager

load_dotenv()


def load_read_articles():
    """è¼‰å…¥å·²è®€æ–‡ç« è¨˜éŒ„"""
    history_file = "tech_news_history.json"
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {"articles": []}
    return {"articles": []}


def save_read_articles(articles_data):
    """å„²å­˜å·²è®€æ–‡ç« è¨˜éŒ„"""
    history_file = "tech_news_history.json"
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(articles_data, f, ensure_ascii=False, indent=2)


def run_daily_tech_news(
    topics: list = None,
    num_articles: int = 7,
    output_file: str = None,
    progress_callback: Optional[callable] = None
):
    """
    åŸ·è¡Œæ¯æ—¥æŠ€è¡“æ–°èæŠ“å–èˆ‡åˆ†æ
    
    Args:
        topics: æ„Ÿèˆˆè¶£çš„ä¸»é¡Œåˆ—è¡¨ï¼ˆé è¨­ç‚º AI ç›¸é—œä¸»é¡Œï¼‰
        num_articles: è¦æ‰¾çš„æ–‡ç« æ•¸é‡ï¼ˆé è¨­ 7 ç¯‡ï¼‰
        output_file: è¼¸å‡ºçš„å ±å‘Šæ–‡ä»¶å
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        
    Returns:
        åŸ·è¡Œçµæœ
    """
    
    # é è¨­ä¸»é¡Œ - å°ˆæ³¨æ–¼ AI é ˜åŸŸ
    if topics is None:
        topics = [
            "Artificial Intelligence", "Machine Learning", "Deep Learning",
            "Large Language Models", "LLM", "GPT", "ChatGPT", "Generative AI",
            "Computer Vision", "Natural Language Processing", "NLP",
            "AI Agents", "AI Tools", "Neural Networks", "Transformer"
        ]
    
    # é è¨­è¼¸å‡ºæ–‡ä»¶å
    if output_file is None:
        today = datetime.now().strftime("%Y%m%d")
        output_file = f"TECH_NEWS_{today}.md"
    
    # åˆå§‹åŒ–æ›´å¿«çš„ LLM
    fast_llm = LLM(
        model="gpt-4o-mini",
        temperature=0.7,
        max_tokens=5000  # å¢åŠ  token æ•¸ä»¥æ”¯æ´æ›´é•·çš„æ‘˜è¦
    )
    
    # åˆå§‹åŒ–å·¥å…·
    search_tool = SerperDevTool()
    scrape_tool = ScrapeWebsiteTool()
    
    # è¼‰å…¥å·²è®€æ–‡ç« 
    history_data = load_read_articles()
    read_urls = [article.get('url', '') for article in history_data.get('articles', [])]
    
    # Agent 1: News Hunter (æ–°èçµäºº)
    news_hunter_backstory = """You are an expert AI and technology news curator with a keen eye for finding 
        the latest and most relevant AI/ML articles and breakthroughs. You excel at searching for recent 
        blog posts, research papers, tutorials, GitHub trending AI repositories, and AI news from reputable 
        sources like ArXiv, Hugging Face, Papers with Code, OpenAI Blog, Google AI Blog, Medium AI publications, 
        Reddit r/MachineLearning, and official AI company blogs. You know how to filter out clickbait and focus 
        on high-quality, educational content about AI that developers and researchers would find valuable."""
    
    enhanced_hunter_backstory = prompt_manager.get_enhanced_backstory(
        'DAILY_TECH_NEWS',
        'news_hunter_prompt',
        news_hunter_backstory
    )
    
    news_hunter = Agent(
        role='AI News Hunter',
        goal=f'Find {num_articles} fresh, high-quality AI/ML articles about: {", ".join(topics[:5])}',
        backstory=enhanced_hunter_backstory,
        tools=[search_tool, scrape_tool],
        llm=fast_llm,
        verbose=True,
        allow_delegation=False
    )
    
    if progress_callback:
        progress_callback("Tech News Hunter", "running", 3, 0)
    
    # Agent 2: Content Analyzer (å…§å®¹åˆ†æå¸«)
    content_analyzer_backstory = """You are an AI/ML technical content analyst who specializes in 
        reading and summarizing AI research papers and technical articles. You can quickly extract 
        key points, identify main AI concepts, model architectures, training techniques, and practical 
        applications from articles. You're skilled at creating DETAILED and COMPREHENSIVE summaries 
        that capture the essence of AI/ML content. Each summary should be at least 200-300 words to 
        provide sufficient context and insights. You also evaluate the quality and relevance of articles."""
    
    enhanced_analyzer_backstory = prompt_manager.get_enhanced_backstory(
        'DAILY_TECH_NEWS',
        'content_analyzer_prompt',
        content_analyzer_backstory
    )
    
    content_analyzer = Agent(
        role='AI Content Analyzer',
        goal='Read and analyze each AI article deeply, extract key insights and create DETAILED summaries (200-300 words each)',
        backstory=enhanced_analyzer_backstory,
        tools=[scrape_tool],
        llm=fast_llm,
        verbose=True,
        allow_delegation=False
    )
    
    # Agent 3: Report Writer (å ±å‘Šæ’°å¯«è€…)
    report_writer_backstory = """You are a skilled AI/ML technical writer who creates engaging 
        daily digests of AI news. You organize information in a clear, scannable format with proper 
        categorization, priority levels, and actionable insights. You know how to present AI/ML 
        content in a way that's both informative and easy to consume, highlighting breakthroughs, 
        new models, research findings, and practical applications."""
    
    enhanced_writer_backstory = prompt_manager.get_enhanced_backstory(
        'DAILY_TECH_NEWS',
        'report_writer_prompt',
        report_writer_backstory
    )
    
    report_writer = Agent(
        role='AI News Report Writer',
        goal='Create a well-organized daily AI/ML news digest report with detailed summaries',
        backstory=enhanced_writer_backstory,
        llm=fast_llm,
        verbose=True,
        allow_delegation=False
    )
    
    # å»ºç«‹å·²è®€ URL åˆ—è¡¨å­—ä¸²
    read_urls_str = "\n".join([f"  - {url}" for url in read_urls[-50:]])  # åªé¡¯ç¤ºæœ€è¿‘ 50 ç­†
    if len(read_urls) > 50:
        read_urls_str += f"\n  ... ä»¥åŠå…¶ä»– {len(read_urls) - 50} ç¯‡æ–‡ç« "
    
    # Task 1: æœå°‹æ–°æ–‡ç« 
    search_task = Task(
        description=f"""Search for {num_articles} NEW and UNIQUE AI/Machine Learning articles published recently 
        (within the last 3-7 days) about the following AI topics: {", ".join(topics[:10])}.

IMPORTANT - FOCUS ON AI/ML CONTENT:
- Prioritize articles about AI models, ML techniques, LLMs, GenAI applications
- Include research papers, model releases, AI tool announcements
- Look for practical AI applications and case studies

IMPORTANT - AVOID DUPLICATE ARTICLES:
The following URLs have already been read and should be EXCLUDED:
{read_urls_str if read_urls else "  (No articles read yet)"}

Search Strategy:
1. Use multiple AI-focused search queries:
   - "latest AI research 2024 2025", "new LLM models", "machine learning breakthrough"
   - "generative AI applications", "AI agents", "computer vision advances"
   - "NLP innovations", "transformer models", "AI tools release"
2. Focus on reputable AI sources:
   - ArXiv.org, Papers with Code
   - Hugging Face Blog, OpenAI Blog, Google AI Blog, DeepMind Blog
   - Medium AI publications (Towards Data Science, etc.)
   - GitHub trending AI repositories
   - Reddit r/MachineLearning, r/artificial
3. For each article, provide:
   - Title
   - URL (must be NEW, not in the excluded list)
   - Source/Author
   - Publish date (if available)
   - Brief description (2-3 sentences about the AI content)

Find diverse AI articles covering different aspects of artificial intelligence.
Ensure ALL {num_articles} articles are UNIQUE and NOT in the excluded list above.
""",
        agent=news_hunter,
        expected_output=f"A list of {num_articles} unique AI/ML articles with titles, URLs, sources, and brief descriptions"
    )
    
    # Task 2: åˆ†ææ–‡ç« å…§å®¹
    analysis_task = Task(
        description=f"""For each AI/ML article found by the News Hunter, visit the URL and perform a detailed analysis:

1. **Read the full article** using the scrape tool
2. **Extract key AI/ML information**:
   - Main AI topic and subtopics
   - AI models, architectures, or techniques mentioned
   - Training methods, datasets, or evaluation metrics (if applicable)
   - Target audience (researcher/practitioner/beginner)
   - Key AI concepts and technologies mentioned
   - Practical applications or use cases
   - Code examples or implementations (if any)
   - Main findings and conclusions
   
3. **Create a DETAILED summary** for each article (MINIMUM 200-300 words) including:
   - Title and URL
   - **Comprehensive 200-300 word summary** explaining:
     * What the article is about
     * Key AI concepts or techniques discussed
     * Main findings or innovations
     * How it relates to current AI trends
     * Practical implications or applications
   - Key AI technologies/models covered
   - Difficulty level (beginner/intermediate/advanced)
   - Why it's valuable for AI practitioners
   - Notable implementations, code, or resources mentioned
   
4. **Rate each article** on:
   - Quality (1-5): Technical depth and accuracy
   - Relevance (1-5): How relevant to current AI trends
   - Practicality (1-5): Immediate applicability
   - Innovation (1-5): Novelty of content

IMPORTANT: Each summary must be DETAILED (200-300 words minimum) to provide sufficient context.
Analyze ALL {num_articles} articles thoroughly with comprehensive summaries.
""",
        agent=content_analyzer,
        expected_output=f"Detailed analysis and COMPREHENSIVE summaries (200-300 words each) for all {num_articles} AI articles with ratings",
        context=[search_task]
    )
    
    # Task 3: ç”Ÿæˆæ¯æ—¥å ±å‘Š
    report_task = Task(
        description=f"""Create a comprehensive daily AI/ML news digest report based on the analyzed articles.

Structure the report as follows:

# ğŸ“° æ¯æ—¥ AI æŠ€è¡“æ–°èæ‘˜è¦ - {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}

## ğŸ“Š ä»Šæ—¥çµ±è¨ˆ
- ğŸ“„ æ–‡ç« ç¸½æ•¸ï¼š{num_articles} ç¯‡
- ğŸ¯ ä¸»è¦é ˜åŸŸï¼š[åˆ—å‡ºä¸»è¦ AI é ˜åŸŸï¼Œå¦‚ LLMã€Computer Vision ç­‰]
- â­ å¹³å‡å“è³ªè©•åˆ†ï¼š[è¨ˆç®—å¹³å‡åˆ†/5]
- ğŸ”¥ ç†±é–€è©±é¡Œï¼š[æå–æœ€å¸¸å‡ºç¾çš„ä¸»é¡Œ]

## ğŸ† ä»Šæ—¥ç²¾é¸æ¨è–¦

For each of the TOP {min(num_articles, 5)} articles (sorted by rating), provide:

### ğŸ“Œ [æ–‡ç« æ¨™é¡Œ]

**åŸºæœ¬è³‡è¨Š**
- ğŸ“ ä¾†æºï¼š[ä½œè€…/ç¶²ç«™/å‡ºç‰ˆæ©Ÿæ§‹]
- ğŸ”— é€£çµï¼š[å®Œæ•´ URL]
- ğŸ·ï¸ é¡åˆ¥ï¼š[LLM/Computer Vision/NLP/ML/ç­‰]
- ğŸ“… ç™¼å¸ƒæ—¥æœŸï¼š[æ—¥æœŸ]
- ğŸ“ é›£åº¦ï¼šâ­â­â­ (åˆç´š/ä¸­ç´š/é«˜ç´š)
- ğŸ“Š è©•åˆ†ï¼šå“è³ª â­â­â­â­â­ | ç›¸é—œæ€§ â­â­â­â­ | å¯¦ç”¨æ€§ â­â­â­â­â­ | å‰µæ–°æ€§ â­â­â­â­

**è©³ç´°æ‘˜è¦** (200-300 words)

[Provide a COMPREHENSIVE summary covering:]
- æ–‡ç« çš„ä¸»è¦å…§å®¹å’Œæ ¸å¿ƒè§€é»
- è¨è«–çš„ AI æ¨¡å‹ã€æŠ€è¡“æˆ–æ–¹æ³•
- ä¸»è¦ç™¼ç¾ã€å‰µæ–°é»æˆ–çªç ´
- å¯¦éš›æ‡‰ç”¨å ´æ™¯æˆ–æ¡ˆä¾‹
- èˆ‡ç•¶å‰ AI è¶¨å‹¢çš„é—œè¯æ€§
- å°è®€è€…çš„åƒ¹å€¼å’Œå•Ÿç™¼

**é—œéµæŠ€è¡“é‡é»**
- ğŸ”§ æŠ€è¡“/æ¨¡å‹ï¼š[åˆ—å‡ºä¸»è¦æŠ€è¡“]
- ğŸ“Š æ–¹æ³•è«–ï¼š[è¨“ç·´æ–¹æ³•ã€è©•ä¼°æŒ‡æ¨™ç­‰]
- ğŸ’¡ å‰µæ–°é»ï¼š[æ–°ç©ä¹‹è™•]

**å¯¦ç”¨è³‡æº**
- ğŸ“¦ ç›¸é—œå¯¦ç¾ï¼š[GitHub é€£çµã€æ¨¡å‹é€£çµç­‰]
- ğŸ“š å»¶ä¼¸é–±è®€ï¼š[ç›¸é—œè«–æ–‡ã€æ–‡ç« ]

---

## ğŸ“š å®Œæ•´æ–‡ç« åˆ—è¡¨

For each remaining article, provide a condensed version:

### [åºè™Ÿ]. [æ–‡ç« æ¨™é¡Œ]
- **ä¾†æº**: [ä¾†æº] | **é›£åº¦**: [é›£åº¦] | **è©•åˆ†**: â­â­â­â­
- **æ‘˜è¦**: [100-150 å­—çš„ç°¡çŸ­æ‘˜è¦]
- **é€£çµ**: [URL]
- **é—œéµå­—**: [ç›¸é—œæŠ€è¡“é—œéµå­—]

---

## ğŸ¯ æœ¬é€±å­¸ç¿’å»ºè­°

Based on today's articles, provide learning recommendations:

1. **å„ªå…ˆå­¸ç¿’ä¸»é¡Œ**: [åŸºæ–¼æ–‡ç« å…§å®¹æ¨è–¦]
   - ç†ç”±ï¼š[ç‚ºä»€éº¼é‡è¦]
   - æ¨è–¦æ–‡ç« ï¼š[å°æ‡‰çš„æ–‡ç« ç·¨è™Ÿ]

2. **å¯¦è¸é …ç›®å»ºè­°**: [å¯ä»¥å‹•æ‰‹åšçš„é …ç›®]
   - ç›¸é—œæ–‡ç« ï¼š[ç·¨è™Ÿ]
   - é›£åº¦è©•ä¼°ï¼š[é›£åº¦]

3. **æ·±å…¥ç ”ç©¶æ–¹å‘**: [å€¼å¾—æ·±å…¥çš„é ˜åŸŸ]
   - å‰æ™¯åˆ†æï¼š[ç°¡è¿°]

## ğŸ”¥ æŠ€è¡“è¶¨å‹¢è§€å¯Ÿ

[æ ¹æ“šä»Šæ—¥æ–‡ç« ï¼Œåˆ†æç•¶å‰ AI é ˜åŸŸçš„ç†±é»è¶¨å‹¢]

## ğŸ“ å¿«é€Ÿç´¢å¼•è¡¨

| # | æ¨™é¡Œ | é¡åˆ¥ | é›£åº¦ | è©•åˆ† | é€£çµ |
|---|------|------|------|------|------|
| 1 | [æ¨™é¡Œç°¡å¯«] | LLM | ä¸­ç´š | â­â­â­â­ | [ğŸ”—](...) |
| 2 | [æ¨™é¡Œç°¡å¯«] | CV | é«˜ç´š | â­â­â­â­â­ | [ğŸ”—](...) |
| ... | ... | ... | ... | ... | ... |

---

**ğŸ“… ç”Ÿæˆæ™‚é–“**ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**ğŸ¤– AI Agents**: News Hunter â†’ Content Analyzer â†’ Report Writer

Use proper markdown formatting with emojis, headers, tables, and lists.
Make it visually appealing and easy to scan.
Save the report to: {output_file}
""",
        agent=report_writer,
        expected_output=f"A well-formatted daily AI news digest with detailed summaries saved as {output_file}",
        context=[search_task, analysis_task],
        output_file=output_file
    )
    
    # å»ºç«‹ Crew
    crew = Crew(
        agents=[news_hunter, content_analyzer, report_writer],
        tasks=[search_task, analysis_task, report_task],
        process=Process.sequential,
        verbose=True
    )
    
    # ä½¿ç”¨åŸ·è¡Œç·’ä¾†æ¨¡æ“¬é€²åº¦æ›´æ–°
    result = None
    error = None
    execution_done = threading.Event()
    
    def run_crew():
        nonlocal result, error
        try:
            result = crew.kickoff()
        except Exception as e:
            error = e
        finally:
            execution_done.set()
    
    # å•Ÿå‹•åŸ·è¡Œç·’
    thread = threading.Thread(target=run_crew, daemon=True)
    thread.start()
    
    # æ¨¡æ“¬é€²åº¦æ›´æ–°ï¼ˆ3 å€‹ Agentsï¼Œå„ç´„ 33% æ™‚é–“ï¼‰
    if progress_callback:
        # éšæ®µ 1: News Hunter
        progress_callback("Tech News Hunter", "running", 3, 0)
        
        for i in range(50):  # æª¢æŸ¥ 50 æ¬¡ï¼Œæ¯æ¬¡ 2 ç§’
            if execution_done.is_set():
                break
            time.sleep(2)
            
            if i == 15:  # ç´„ 33% æ™‚é–“
                progress_callback("Tech News Hunter", "completed", 3, 1)
                progress_callback("Technical Content Analyzer", "running", 3, 1)
            elif i == 35:  # ç´„ 66% æ™‚é–“
                progress_callback("Technical Content Analyzer", "completed", 3, 2)
                progress_callback("Tech News Report Writer", "running", 3, 2)
    
    # ç­‰å¾…åŸ·è¡Œå®Œæˆ
    execution_done.wait()
    
    # é¡¯ç¤ºå®Œæˆ
    if progress_callback:
        progress_callback("Tech News Report Writer", "completed", 3, 3)
    
    # å¦‚æœæœ‰éŒ¯èª¤ï¼Œæ‹‹å‡º
    if error:
        raise error
    
    # æ›´æ–°å·²è®€æ–‡ç« è¨˜éŒ„ï¼ˆå¾çµæœä¸­æå– URLsï¼‰
    # æ³¨æ„ï¼šé€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²å¾çµæœä¸­è§£æå‡ºæ–°çš„ URLs
    
    return result
